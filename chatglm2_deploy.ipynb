{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a63c0138-d19f-4109-aac5-fd0f028a5869",
   "metadata": {},
   "source": [
    "### 1.安装 HuggingFace 并下载模型到本地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efb1a261-748a-470a-900f-1863db83447b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install huggingface-hub -Uq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae8f333d-7708-4ffc-8887-55e9bff44129",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "local_model_path = Path(\"./LLM_chatglm2_model\")\n",
    "local_model_path.mkdir(exist_ok=True)\n",
    "model_name = \"THUDM/chatglm2-6b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9dd16c8-e2f7-4d59-8d10-b763a28b6a3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c296a1ed2c624bfeabf3bca339ebf292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 18 files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "553ae1ce722f49abb5fd04ad8109930b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)2a579/.gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b18c556dc7a413e96896c51ada8a610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)a6c2a579/config.json:   0%|          | 0.00/1.14k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cafebd3b6b9c45ac97ec04ea7572432f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)c2a579/MODEL_LICENSE:   0%|          | 0.00/2.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e20ef4e8d234c3d94de781ee5eb9aea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)iguration_chatglm.py:   0%|          | 0.00/2.08k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "599cbb29559d46b989023be74d561e8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)5fa6c2a579/README.md:   0%|          | 0.00/7.69k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df67bb7f7beb4aef9742f51ec1c4631b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/modeling_chatglm.py:   0%|          | 0.00/47.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c9397cea79042478d6c30e393a1b337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/20.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71debd315bc24b2c911c7bcccce608c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00004-of-00007.bin:   0%|          | 0.00/1.82G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18c3e838e5f94c98af5512fd0e30fabb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00003-of-00007.bin:   0%|          | 0.00/1.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26efd7a977dc43afb0d3d5ac5868425d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00007-of-00007.bin:   0%|          | 0.00/1.05G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba7f77c74cab4c1a90ac578c00695d2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)a579/quantization.py:   0%|          | 0.00/14.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d4a3f71617d47ef8f2cc7fff6d4afd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00005-of-00007.bin:   0%|          | 0.00/1.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dc76d3bd4964e5da24fb52d4efd7d32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00007.bin:   0%|          | 0.00/1.83G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a212756f44bb4755b8956b19cf4db327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00007.bin:   0%|          | 0.00/1.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa0e26946211441a99269bb703ad522e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00006-of-00007.bin:   0%|          | 0.00/1.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68add4ec86b94508a7938dd5de7b3bfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)enization_chatglm.py:   0%|          | 0.00/9.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0159f782076a43cca85cd3b5279d8ba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.model:   0%|          | 0.00/1.02M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6620cabc6404438b4271ac14a24e0f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/244 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'LLM_chatglm2_model/models--THUDM--chatglm2-6b/snapshots/e186c891cf64310ac66ef10a87e6635fa6c2a579'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snapshot_download(repo_id=model_name, cache_dir=local_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbeea66-a864-4403-984a-e1f86b9958ab",
   "metadata": {},
   "source": [
    "### 2.SageMaker 初始化配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0660d52d-9c07-4445-893b-d95a2227eeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import os\n",
    "from sagemaker import image_uris\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "region = sess._region_name\n",
    "account_id = sess.account_id()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65185ac-7a43-4661-9fcb-7d77541a04d2",
   "metadata": {},
   "source": [
    "### 3. 把模型拷贝到 S3 存储桶为后续部署做准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bf8cccf-f3b7-4d34-ab14-be364901c1ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3_code_prefix: LLM_chatglm2_deploy_code\n",
      "model_snapshot_path: LLM_chatglm2_model/models--THUDM--chatglm2-6b/snapshots/e186c891cf64310ac66ef10a87e6635fa6c2a579\n"
     ]
    }
   ],
   "source": [
    "s3_model_prefix = \"LLM_chatglm2_model\"  # folder where model checkpoint will go\n",
    "model_snapshot_path = list(local_model_path.glob(\"**/snapshots/*\"))[0]\n",
    "s3_code_prefix = \"LLM_chatglm2_deploy_code\"\n",
    "\n",
    "print(f\"s3_code_prefix: {s3_code_prefix}\")\n",
    "print(f\"model_snapshot_path: {model_snapshot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "068e3ed1-9a9b-4ef2-92dd-4afcd9c1ff3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "for root, dirs, files in os.walk(model_snapshot_path):\n",
    "    for file in files:\n",
    "        local_path = os.path.join(root, file)\n",
    "        s3_key = s3_model_prefix + '/' + os.path.relpath(local_path, model_snapshot_path)\n",
    "        s3_client.upload_file(local_path, bucket, s3_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0527e45-c2da-477b-91fb-8d878a735ca2",
   "metadata": {},
   "source": [
    "### 3.模型部署准备"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efb3b82-d8c5-41d3-86ac-6ebfeb7e01c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "* 推理容器镜像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8269e86f-d50f-4857-a079-1a7a67f8e038",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image going to be used is ---- > 763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.22.1-deepspeed0.9.2-cu118\n"
     ]
    }
   ],
   "source": [
    "inference_image_uri = (\n",
    "    f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.22.1-deepspeed0.9.2-cu118\"\n",
    ")\n",
    "\n",
    "# 中国区需要替换为下面的image_uri\n",
    "# inference_image_uri = (\n",
    "#     f\"727897471807.dkr.ecr.{region}.amazonaws.com.cn/djl-inference:0.22.1-deepspeed0.9.2-cu118\"\n",
    "# )\n",
    "\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faaf6561-3a6f-4482-8190-909980e9f8ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chatglm2_deploy_code_path = Path(\"./LLM_chatglm2_deploy_code\")\n",
    "chatglm2_deploy_code_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f93edb-2a6a-4951-a4c6-4dbfdec4152b",
   "metadata": {},
   "source": [
    "* Entrypoint 脚本 model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cba608c3-b401-4978-9c42-7bee13e209e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing LLM_chatglm2_deploy_code/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile LLM_chatglm2_deploy_code/model.py\n",
    "from djl_python import Input, Output\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import logging\n",
    "\n",
    "def load_model(properties):\n",
    "    tensor_parallel = properties[\"tensor_parallel_degree\"]\n",
    "    model_location = properties['model_dir']\n",
    "    if \"model_id\" in properties:\n",
    "        model_location = properties['model_id']\n",
    "    logging.info(f\"Loading model in {model_location}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_location, trust_remote_code=True)\n",
    "    model = AutoModel.from_pretrained(model_location, trust_remote_code=True).half().cuda()\n",
    "    model.eval()\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "model = None\n",
    "tokenizer = None\n",
    "\n",
    "def handle(inputs: Input):\n",
    "    global model, tokenizer\n",
    "    if not model:\n",
    "        model, tokenizer = load_model(inputs.get_properties())\n",
    "\n",
    "    if inputs.is_empty():\n",
    "        return None\n",
    "    data = inputs.get_as_json()\n",
    "    \n",
    "    input_sentences = data[\"inputs\"]\n",
    "    params = data[\"parameters\"]\n",
    "    history = data[\"history\"]\n",
    "    \n",
    "    response, history = model.chat(tokenizer, input_sentences, history=history, **params)\n",
    "    \n",
    "    result = {\"outputs\": response, \"history\" : history}\n",
    "    return Output().add_as_json(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada3b51f-03ee-4452-a974-aeccc68512e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "* serving.properties 配置文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6af1838-0e93-416a-9f0a-27641d8736a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "option.s3url ==> s3://sagemaker-us-east-1-091166060467/LLM_chatglm2_model/\n"
     ]
    }
   ],
   "source": [
    "print(f\"option.s3url ==> s3://{bucket}/{s3_model_prefix}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677ba16e-a749-4f6f-8566-d71b8afae8de",
   "metadata": {},
   "source": [
    "> 需要修改按照上述步骤的 s3url 修改 option.s3url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "834150fe-a3cb-4d05-b479-ad73907cc6d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing LLM_chatglm2_deploy_code/serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%writefile LLM_chatglm2_deploy_code/serving.properties\n",
    "engine=Python\n",
    "option.tensor_parallel_degree=1\n",
    "option.s3url = s3://sagemaker-us-east-1-091166060467/LLM_chatglm2_model/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b6a5a6-8c04-4e2c-ab53-b0e96082c14d",
   "metadata": {},
   "source": [
    "* 将配置文件压缩后上传 S3 存储桶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4560c8a0-6e24-4ee0-bd89-f6673c0b88b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "folder_path = 'LLM_chatglm2_deploy_code'\n",
    "output_filename = 'model.tar.gz'\n",
    "\n",
    "with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "    tar.add(folder_path, arcname=os.path.basename(folder_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0ef2e7b-411d-4b97-8d20-f36ab15a56b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Code or Model tar ball uploaded to --- > s3://sagemaker-us-east-1-091166060467/LLM_chatglm2_deploy_code/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "s3_code_artifact = sess.upload_data(\"model.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {s3_code_artifact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c95d83b-996e-4e7a-a84d-9b16406496be",
   "metadata": {},
   "source": [
    "### 4. 模型部署"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abc0dc85-2760-4132-9a60-9da0eb77ffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "\n",
    "def create_model(model_name, model_s3_url):\n",
    "    model = Model(\n",
    "        image_uri=inference_image_uri,\n",
    "        model_data=model_s3_url,\n",
    "        role=role,\n",
    "        name=model_name,\n",
    "        sagemaker_session=sess,\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ed2e95a-54ca-4b9e-a479-9d3da31079ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import serializers, deserializers\n",
    "\n",
    "def deploy_model(model, _endpoint_name):\n",
    "    model.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type=\"ml.g4dn.2xlarge\",\n",
    "        endpoint_name=_endpoint_name\n",
    "    )\n",
    "    predictor = sagemaker.Predictor(\n",
    "        endpoint_name=_endpoint_name,\n",
    "        sagemaker_session=sess,\n",
    "        serializer=serializers.JSONSerializer(),\n",
    "        deserializer=deserializers.JSONDeserializer()\n",
    "    )\n",
    "    return predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2dacdec-395c-4144-aa38-5e706faa1dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "_model_name = name_from_base(f\"chatglm2\") # Append a timestamp to the provided string\n",
    "_model_s3_url = s3_code_artifact\n",
    "_endpoint_name = f\"{_model_name}-endpoint\"\n",
    "\n",
    "model = create_model(_model_name, _model_s3_url)\n",
    "predictor = deploy_model(model, _endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfd0e0a-bd9c-41cd-9e0b-ce1641542076",
   "metadata": {},
   "source": [
    "### 5. 模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a82625f-cee0-409c-9060-be8620432ae8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "  \"max_length\": 4096,\n",
    "  \"temperature\": 0.01,\n",
    "  \"top_p\": 0.7,\n",
    "}\n",
    "history = [['你是气象专家智能问答机器人，了解各种气象知识和气象信息，可以自由回答问题，像人类一样思考和表达。当我向你提问时你必须使用，“您好，我是气象专家智能问答机器人”这句话作为开头','好的']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff94d396-55f2-49d7-8eed-63333d478c4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompts1 = \"\"\"北京是不是夏天雨水比较多？\"\"\"\n",
    "\n",
    "reponse = predictor.predict(\n",
    "    {\n",
    "        \"inputs\" : prompts1, \n",
    "        \"parameters\": parameters,\n",
    "        \"history\" : history\n",
    "    }\n",
    ")\n",
    "history.extend(reponse['history'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4af65abb-eb1e-4afa-9a9c-483a8805b1f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'outputs': '您好，我是气象专家智能问答机器人。北京属于温带季风气候，夏季气温较高，降雨量较大。因此，北京在夏季雨水较多。', 'history': [['你是气象专家智能问答机器人，了解各种气象知识和气象信息，可以自由回答问题，像人类一样思考和表达。当我向你提问时你必须使用，“您好，我是气象专家智能问答机器人”这句话作为开头', '好的'], ['北京是不是夏天雨水比较多？', '您好，我是气象专家智能问答机器人。北京属于温带季风气候，夏季气温较高，降雨量较大。因此，北京在夏季雨水较多。']]}\n"
     ]
    }
   ],
   "source": [
    "print(reponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea4ea396-ac91-484f-b752-500e39c93bc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(reponse['outputs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce65d9d7-7646-4833-9915-e0f7a632a4ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompts2 = \"\"\"你说的是真的吗？举个具体例子吧\"\"\"\n",
    "\n",
    "reponse = predictor.predict(\n",
    "    {\n",
    "        \"inputs\" : prompts2, \n",
    "        \"parameters\": parameters,\n",
    "        \"history\" : history\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c2fbaec-72ed-4920-9ec2-3fe5704b52b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'outputs': '好的，比如2021年7月，北京平均降水量达到了181.1毫米，属于历史同期最高值。', 'history': [['你是气象专家智能问答机器人，了解各种气象知识和气象信息，可以自由回答问题，像人类一样思考和表达。当我向你提问时你必须使用，“您好，我是气象专家智能问答机器人”这句话作为开头', '好的'], ['你是气象专家智能问答机器人，了解各种气象知识和气象信息，可以自由回答问题，像人类一样思考和表达。当我向你提问时你必须使用，“您好，我是气象专家智能问答机器人”这句话作为开头', '好的'], ['北京是不是夏天雨水比较多？', '您好，我是气象专家智能问答机器人。北京属于温带季风气候，夏季气温较高，降雨量较大。因此，北京在夏季雨水较多。'], ['你说的是真的吗？举个具体例子吧', '好的，比如2021年7月，北京平均降水量达到了181.1毫米，属于历史同期最高值。']]}\n"
     ]
    }
   ],
   "source": [
    "print(reponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "708da5a4-3ed9-44af-b5e6-b7c165dee2f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(reponse['outputs'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
