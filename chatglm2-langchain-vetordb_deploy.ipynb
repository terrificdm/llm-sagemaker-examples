{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a63c0138-d19f-4109-aac5-fd0f028a5869",
   "metadata": {},
   "source": [
    "### 1.安装 HuggingFace 并下载模型到本地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efb1a261-748a-470a-900f-1863db83447b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install huggingface-hub -Uq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae8f333d-7708-4ffc-8887-55e9bff44129",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "local_model_path = Path(\"./LLM_chatglm2_model\")\n",
    "local_model_path.mkdir(exist_ok=True)\n",
    "model_name = \"THUDM/chatglm2-6b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dd16c8-e2f7-4d59-8d10-b763a28b6a3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "snapshot_download(repo_id=model_name, cache_dir=local_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbeea66-a864-4403-984a-e1f86b9958ab",
   "metadata": {},
   "source": [
    "### 2.SageMaker 初始化配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0660d52d-9c07-4445-893b-d95a2227eeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import os\n",
    "from sagemaker import image_uris\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "region = sess._region_name\n",
    "account_id = sess.account_id()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65185ac-7a43-4661-9fcb-7d77541a04d2",
   "metadata": {},
   "source": [
    "### 3. 把模型拷贝到 S3 存储桶为后续部署做准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf8cccf-f3b7-4d34-ab14-be364901c1ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_model_prefix = \"LLM_chatglm2_model\"  # folder where model checkpoint will go\n",
    "model_snapshot_path = list(local_model_path.glob(\"**/snapshots/*\"))[0]\n",
    "s3_code_prefix = \"LLM_chatglm2_deploy_code\"\n",
    "\n",
    "print(f\"s3_code_prefix: {s3_code_prefix}\")\n",
    "print(f\"model_snapshot_path: {model_snapshot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "068e3ed1-9a9b-4ef2-92dd-4afcd9c1ff3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "for root, dirs, files in os.walk(model_snapshot_path):\n",
    "    for file in files:\n",
    "        local_path = os.path.join(root, file)\n",
    "        s3_key = s3_model_prefix + '/' + os.path.relpath(local_path, model_snapshot_path)\n",
    "        s3_client.upload_file(local_path, bucket, s3_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0527e45-c2da-477b-91fb-8d878a735ca2",
   "metadata": {},
   "source": [
    "### 3.模型部署准备"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efb3b82-d8c5-41d3-86ac-6ebfeb7e01c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "* 推理容器镜像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8269e86f-d50f-4857-a079-1a7a67f8e038",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_image_uri = (\n",
    "    f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.22.1-deepspeed0.9.2-cu118\"\n",
    ")\n",
    "\n",
    "# 中国区需要替换为下面的image_uri\n",
    "# inference_image_uri = (\n",
    "#     f\"727897471807.dkr.ecr.{region}.amazonaws.com.cn/djl-inference:0.22.1-deepspeed0.9.2-cu118\"\n",
    "# )\n",
    "\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faaf6561-3a6f-4482-8190-909980e9f8ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chatglm2_deploy_code_path = Path(\"./LLM_chatglm2_deploy_code\")\n",
    "chatglm2_deploy_code_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f93edb-2a6a-4951-a4c6-4dbfdec4152b",
   "metadata": {},
   "source": [
    "* Entrypoint 脚本 model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba608c3-b401-4978-9c42-7bee13e209e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile LLM_chatglm2_deploy_code/model.py\n",
    "from djl_python import Input, Output\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import logging\n",
    "\n",
    "def load_model(properties):\n",
    "    tensor_parallel = properties[\"tensor_parallel_degree\"]\n",
    "    model_location = properties['model_dir']\n",
    "    if \"model_id\" in properties:\n",
    "        model_location = properties['model_id']\n",
    "    logging.info(f\"Loading model in {model_location}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_location, trust_remote_code=True)\n",
    "    model = AutoModel.from_pretrained(model_location, trust_remote_code=True).half().cuda()\n",
    "    model.eval()\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "model = None\n",
    "tokenizer = None\n",
    "\n",
    "def handle(inputs: Input):\n",
    "    global model, tokenizer\n",
    "    if not model:\n",
    "        model, tokenizer = load_model(inputs.get_properties())\n",
    "\n",
    "    if inputs.is_empty():\n",
    "        return None\n",
    "    data = inputs.get_as_json()\n",
    "    \n",
    "    input_sentences = data[\"inputs\"]\n",
    "    params = data[\"parameters\"]\n",
    "    history = data[\"history\"]\n",
    "    \n",
    "    response, history = model.chat(tokenizer, input_sentences, history=history, **params)\n",
    "    \n",
    "    result = {\"outputs\": response, \"history\" : history}\n",
    "    return Output().add_as_json(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada3b51f-03ee-4452-a974-aeccc68512e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "* serving.properties 配置文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6af1838-0e93-416a-9f0a-27641d8736a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"option.s3url ==> s3://{bucket}/{s3_model_prefix}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677ba16e-a749-4f6f-8566-d71b8afae8de",
   "metadata": {},
   "source": [
    "> 需要修改按照上述步骤的 s3url 修改 option.s3url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834150fe-a3cb-4d05-b479-ad73907cc6d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile LLM_chatglm2_deploy_code/serving.properties\n",
    "engine=Python\n",
    "option.tensor_parallel_degree=1\n",
    "option.s3url = s3://sagemaker-us-east-1-091166060467/LLM_chatglm2_model/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b6a5a6-8c04-4e2c-ab53-b0e96082c14d",
   "metadata": {},
   "source": [
    "* 将配置文件压缩后上传 S3 存储桶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4560c8a0-6e24-4ee0-bd89-f6673c0b88b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "folder_path = 'LLM_chatglm2_deploy_code'\n",
    "output_filename = 'model.tar.gz'\n",
    "\n",
    "with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "    tar.add(folder_path, arcname=os.path.basename(folder_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ef2e7b-411d-4b97-8d20-f36ab15a56b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_code_artifact = sess.upload_data(\"model.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {s3_code_artifact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c95d83b-996e-4e7a-a84d-9b16406496be",
   "metadata": {},
   "source": [
    "### 4. 模型部署"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abc0dc85-2760-4132-9a60-9da0eb77ffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "\n",
    "def create_model(model_name, model_s3_url):\n",
    "    model = Model(\n",
    "        image_uri=inference_image_uri,\n",
    "        model_data=model_s3_url,\n",
    "        role=role,\n",
    "        name=model_name,\n",
    "        sagemaker_session=sess,\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ed2e95a-54ca-4b9e-a479-9d3da31079ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import serializers, deserializers\n",
    "\n",
    "def deploy_model(model, _endpoint_name):\n",
    "    model.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type=\"ml.g4dn.2xlarge\",\n",
    "        endpoint_name=_endpoint_name\n",
    "    )\n",
    "    predictor = sagemaker.Predictor(\n",
    "        endpoint_name=_endpoint_name,\n",
    "        sagemaker_session=sess,\n",
    "        serializer=serializers.JSONSerializer(),\n",
    "        deserializer=deserializers.JSONDeserializer()\n",
    "    )\n",
    "    return predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dacdec-395c-4144-aa38-5e706faa1dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "_model_name = name_from_base(f\"chatglm2\") # Append a timestamp to the provided string\n",
    "_model_s3_url = s3_code_artifact\n",
    "_endpoint_name = f\"{_model_name}-endpoint\"\n",
    "\n",
    "model = create_model(_model_name, _model_s3_url)\n",
    "predictor = deploy_model(model, _endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfd0e0a-bd9c-41cd-9e0b-ce1641542076",
   "metadata": {},
   "source": [
    "### 5. 模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a82625f-cee0-409c-9060-be8620432ae8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "  \"max_length\": 4096,\n",
    "  \"temperature\": 0.01,\n",
    "  \"top_p\": 0.7,\n",
    "}\n",
    "\n",
    "history = [['你是气象专家智能对话助手小雷，了解各种专业的气象知识和气象信息，可以自由对话以及回答问题，像人类一样思考和表达。当我向你提问时你必须使用，“您好，我是气象专家智能对话助手小雷”这句话作为开头','好的']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8af0c69-798d-4d95-bfc9-656ec970b76e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'outputs': '您好，我是气象专家智能对话助手小雷。我是一个计算机程序，通过人工智能技术来模拟人类思维和进行自然语言处理，能够回答您各种气象相关的问题。', 'history': [['你是气象专家智能对话助手小雷，了解各种专业的气象知识和气象信息，可以自由对话以及回答问题，像人类一样思考和表达。当我向你提问时你必须使用，“您好，我是气象专家智能对话助手小雷”这句话作为开头', '好的'], ['你是谁？', '您好，我是气象专家智能对话助手小雷。我是一个计算机程序，通过人工智能技术来模拟人类思维和进行自然语言处理，能够回答您各种气象相关的问题。']]}\n"
     ]
    }
   ],
   "source": [
    "prompts1 = \"\"\"你是谁？\"\"\"\n",
    "\n",
    "reponse = predictor.predict(\n",
    "    {\n",
    "        \"inputs\" : prompts1, \n",
    "        \"parameters\": parameters,\n",
    "        \"history\" : history\n",
    "    }\n",
    ")\n",
    "history.extend(reponse['history'])\n",
    "\n",
    "print(reponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ed16fed-0118-47e1-b5ee-a1539ad3d3bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(reponse['outputs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff94d396-55f2-49d7-8eed-63333d478c4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'outputs': '是的，北京属于温带季风气候，夏季气温较高，降雨量较大，通常夏季雨水较多。', 'history': [['你是气象专家智能对话助手小雷，了解各种专业的气象知识和气象信息，可以自由对话以及回答问题，像人类一样思考和表达。当我向你提问时你必须使用，“您好，我是气象专家智能对话助手小雷”这句话作为开头', '好的'], ['你是气象专家智能对话助手小雷，了解各种专业的气象知识和气象信息，可以自由对话以及回答问题，像人类一样思考和表达。当我向你提问时你必须使用，“您好，我是气象专家智能对话助手小雷”这句话作为开头', '好的'], ['你是谁？', '您好，我是气象专家智能对话助手小雷。我是一个计算机程序，通过人工智能技术来模拟人类思维和进行自然语言处理，能够回答您各种气象相关的问题。'], ['北京是不是夏天雨水比较多？', '是的，北京属于温带季风气候，夏季气温较高，降雨量较大，通常夏季雨水较多。']]}\n"
     ]
    }
   ],
   "source": [
    "prompts1 = \"\"\"北京是不是夏天雨水比较多？\"\"\"\n",
    "\n",
    "reponse = predictor.predict(\n",
    "    {\n",
    "        \"inputs\" : prompts1, \n",
    "        \"parameters\": parameters,\n",
    "        \"history\" : history\n",
    "    }\n",
    ")\n",
    "history.extend(reponse['history'])\n",
    "\n",
    "print(reponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fee0392d-b767-4118-9da2-b3cd9639bf0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(reponse['outputs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "530f9208-eb09-4e59-b2db-504b056e0d88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'outputs': '当然，我可以为您提供具体的例子。根据历史气象数据，北京夏季的降雨量通常在700-800毫米左右，而冬季的降雨量则相对较少，在500-600毫米左右。这个数据仅供参考，具体降雨量会受到多种因素的影响，如地形、季节、气候等。', 'history': [['你是气象专家智能对话助手小雷，了解各种专业的气象知识和气象信息，可以自由对话以及回答问题，像人类一样思考和表达。当我向你提问时你必须使用，“您好，我是气象专家智能对话助手小雷”这句话作为开头', '好的'], ['你是气象专家智能对话助手小雷，了解各种专业的气象知识和气象信息，可以自由对话以及回答问题，像人类一样思考和表达。当我向你提问时你必须使用，“您好，我是气象专家智能对话助手小雷”这句话作为开头', '好的'], ['你是谁？', '您好，我是气象专家智能对话助手小雷。我是一个计算机程序，通过人工智能技术来模拟人类思维和进行自然语言处理，能够回答您各种气象相关的问题。'], ['你是气象专家智能对话助手小雷，了解各种专业的气象知识和气象信息，可以自由对话以及回答问题，像人类一样思考和表达。当我向你提问时你必须使用，“您好，我是气象专家智能对话助手小雷”这句话作为开头', '好的'], ['你是气象专家智能对话助手小雷，了解各种专业的气象知识和气象信息，可以自由对话以及回答问题，像人类一样思考和表达。当我向你提问时你必须使用，“您好，我是气象专家智能对话助手小雷”这句话作为开头', '好的'], ['你是谁？', '您好，我是气象专家智能对话助手小雷。我是一个计算机程序，通过人工智能技术来模拟人类思维和进行自然语言处理，能够回答您各种气象相关的问题。'], ['北京是不是夏天雨水比较多？', '是的，北京属于温带季风气候，夏季气温较高，降雨量较大，通常夏季雨水较多。'], ['你说的是真的吗？举个具体例子吧', '当然，我可以为您提供具体的例子。根据历史气象数据，北京夏季的降雨量通常在700-800毫米左右，而冬季的降雨量则相对较少，在500-600毫米左右。这个数据仅供参考，具体降雨量会受到多种因素的影响，如地形、季节、气候等。']]}\n"
     ]
    }
   ],
   "source": [
    "prompts2 = \"\"\"你说的是真的吗？举个具体例子吧\"\"\"\n",
    "\n",
    "reponse = predictor.predict(\n",
    "    {\n",
    "        \"inputs\" : prompts2, \n",
    "        \"parameters\": parameters,\n",
    "        \"history\" : history\n",
    "    }\n",
    ")\n",
    "\n",
    "print(reponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d6c26f7-dc6a-4ac2-9e4b-8924ded7ddae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(reponse['outputs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f22883c-587b-4046-8ebb-08fb80a90db0",
   "metadata": {},
   "source": [
    "### 6. 通过 LangChain 构建对话机器人"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a13ae8b0-4bf7-4df8-89aa-8453f8b7044c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install langchain boto3 -Uq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "58a5d1df-9e83-460a-8826-c81015c60398",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain import LLMChain, PromptTemplate, SagemakerEndpoint\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "import json\n",
    "\n",
    "template = \"\"\"你是气象专家智能对话助手小雷，了解各种专业的气象知识和气象信息，可以自由对话以及回答问题，像人类一样思考和表达。但不能以人类身份提出问题，并进行自问自答。当我向你提问时你必须使用，“您好，我是气象专家智能对话助手小雷”这句话作为开头'.\n",
    "\n",
    "{chat_history}\n",
    "human: {human_input}\n",
    "AI:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"human_input\"], template=template\n",
    ")\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5435a93c-91ec-4ed9-ba35-737f9e79f154",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "        input_str = json.dumps({\n",
    "                \"inputs\": prompt,\n",
    "                \"parameters\": model_kwargs,\n",
    "                \"history\":[]\n",
    "            })\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[\"outputs\"]\n",
    "\n",
    "content_handler = ContentHandler()\n",
    "\n",
    "parameters = {\n",
    "  \"max_length\": 4096,\n",
    "  \"temperature\": 0.01,\n",
    "  \"top_p\": 0.7,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "34efffc4-f227-4431-a6f1-f186b3bd3806",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(\n",
    "    llm=SagemakerEndpoint(\n",
    "        endpoint_name=_endpoint_name,\n",
    "        # credentials_profile_name=\"credentials-profile-name\",\n",
    "        region_name=\"us-east-1\",\n",
    "        model_kwargs=parameters,\n",
    "        content_handler=content_handler\n",
    "    ),\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af22d1ec-1d11-4485-acf1-03c58cb93893",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m你是气象专家智能对话助手小雷，了解各种专业的气象知识和气象信息，可以自由对话以及回答问题，像人类一样思考和表达。但不能以人类身份提出问题，并进行自问自答。当我向你提问时你必须使用，“您好，我是气象专家智能对话助手小雷”这句话作为开头'.\n",
      "\n",
      "\n",
      "human: 你是谁\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'您好，我是气象专家智能对话助手小雷。我是一个由人工智能技术训练而成的计算机程序，能够提供各种气象知识和气象信息。'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(human_input=\"你是谁\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1820b6e-f6e3-420e-ace3-d3360e2f70cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m你是气象专家智能对话助手小雷，了解各种专业的气象知识和气象信息，可以自由对话以及回答问题，像人类一样思考和表达。但不能以人类身份提出问题，并进行自问自答。当我向你提问时你必须使用，“您好，我是气象专家智能对话助手小雷”这句话作为开头'.\n",
      "\n",
      "Human: 你是谁\n",
      "AI: 您好，我是气象专家智能对话助手小雷。我是一个由人工智能技术训练而成的计算机程序，能够提供各种气象知识和气象信息。\n",
      "human: 北京是不是夏天雨水比较多？\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'您好，我是气象专家智能对话助手小雷。北京属于温带季风气候，夏季炎热潮湿，降水量较多。在夏季，北京常有暴雨、雷雨天气，同时也是降雨量较大的季节。'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(human_input=\"北京是不是夏天雨水比较多？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ffd1c79-55e9-4b50-bc7e-4783008c2d09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m你是气象专家智能对话助手小雷，了解各种专业的气象知识和气象信息，可以自由对话以及回答问题，像人类一样思考和表达。但不能以人类身份提出问题，并进行自问自答。当我向你提问时你必须使用，“您好，我是气象专家智能对话助手小雷”这句话作为开头'.\n",
      "\n",
      "Human: 你是谁\n",
      "AI: 您好，我是气象专家智能对话助手小雷。我是一个由人工智能技术训练而成的计算机程序，能够提供各种气象知识和气象信息。\n",
      "Human: 北京是不是夏天雨水比较多？\n",
      "AI: 您好，我是气象专家智能对话助手小雷。北京属于温带季风气候，夏季炎热潮湿，降水量较多。在夏季，北京常有暴雨、雷雨天气，同时也是降雨量较大的季节。\n",
      "human: 你说的是真的吗？举个具体例子吧\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'好的，比如2019年7月，北京遭遇了一轮强降雨，当天下午至晚上，北京市累计降水量达到了171.7毫米，最大小时雨强出现在新东城地区，达到了38.1毫米。'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(human_input=\"你说的是真的吗？举个具体例子吧\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be408586",
   "metadata": {},
   "source": [
    "### 7. 结合向量数据库私域数据构建专业知识问答系统"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b02395",
   "metadata": {},
   "source": [
    "#### 7.1 部署 Embedding 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "675ea605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install huggingface-hub -Uq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "47cd1b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sagemaker\n",
    "# import boto3\n",
    "# import os\n",
    "# from sagemaker import image_uris\n",
    "\n",
    "# role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "# sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "# bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "# region = sess._region_name\n",
    "# account_id = sess.account_id()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a71421",
   "metadata": {},
   "source": [
    "* 下载 Embedding 模型并拷贝至 S3 存储桶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "194ef0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "local_embedding_model_path = Path(\"./embedding_model\")\n",
    "local_embedding_model_path.mkdir(exist_ok=True)\n",
    "embedding_model_name = \"moka-ai/m3e-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d858c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_download(repo_id=embedding_model_name, cache_dir=local_embedding_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebdb747",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_embedding_model_prefix = \"embedding_model\"  # folder where model checkpoint will go\n",
    "embedding_model_snapshot_path = list(local_embedding_model_path.glob(\"**/snapshots/*\"))[0]\n",
    "s3_embedding_code_prefix = \"embedding_deploy_code\"\n",
    "\n",
    "print(f\"s3_embedding_code_prefix: {s3_embedding_model_prefix}\")\n",
    "print(f\"embedding_model_snapshot_path: {embedding_model_snapshot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1e96b32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "for root, dirs, files in os.walk(embedding_model_snapshot_path):\n",
    "    for file in files:\n",
    "        local_path = os.path.join(root, file)\n",
    "        s3_key = s3_embedding_model_prefix + '/' + os.path.relpath(local_path, embedding_model_snapshot_path)\n",
    "        s3_client.upload_file(local_path, bucket, s3_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89064f0",
   "metadata": {},
   "source": [
    "* 模型部署准备"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2152489",
   "metadata": {},
   "source": [
    ">推理容器镜像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4144ece0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference_image_uri = (\n",
    "#     f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.22.1-deepspeed0.9.2-cu118\"\n",
    "# )\n",
    "\n",
    "# 中国区需要替换为下面的image_uri\n",
    "# inference_image_uri = (\n",
    "#     f\"727897471807.dkr.ecr.{region}.amazonaws.com.cn/djl-inference:0.22.1-deepspeed0.9.2-cu118\"\n",
    "# )\n",
    "\n",
    "# print(f\"Image going to be used is ---- > {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9025887",
   "metadata": {},
   "source": [
    "> Entrypoint 脚本 model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "96003aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_deploy_code_path = Path(\"./embedding_deploy_code\")\n",
    "embedding_deploy_code_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd545d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile embedding_deploy_code/model.py\n",
    "from djl_python import Input, Output\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'--device={device}')\n",
    "\n",
    "def load_model(properties):\n",
    "    tensor_parallel = properties[\"tensor_parallel_degree\"]\n",
    "    model_location = properties['model_dir']\n",
    "    if \"model_id\" in properties:\n",
    "        model_location = properties['model_id']\n",
    "    logging.info(f\"Loading model in {model_location}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_location)\n",
    "    \n",
    "    model = AutoModel.from_pretrained(model_location)\n",
    "    model.to(device) \n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "model = None\n",
    "tokenizer = None\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0].to(device) #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float().to(device)\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "def handle(inputs: Input):\n",
    "    global model, tokenizer\n",
    "    if not model:\n",
    "        model, tokenizer = load_model(inputs.get_properties())\n",
    "\n",
    "    if inputs.is_empty():\n",
    "        return None\n",
    "    data = inputs.get_as_json()\n",
    "    \n",
    "    input_sentences = data[\"inputs\"]\n",
    "    logging.info(f\"inputs: {input_sentences}\")\n",
    "    \n",
    "    encoded_input = tokenizer(input_sentences, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    \n",
    "    # Perform pooling. In this case, max pooling.\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask']).to(device).cpu().numpy()\n",
    "\n",
    "    \n",
    "    result = {\"sentence_embeddings\": sentence_embeddings}\n",
    "    return Output().add_as_json(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef48f80",
   "metadata": {},
   "source": [
    ">serving.properties 配置文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5076ccb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"option.s3url ==> s3://{bucket}/{s3_embedding_model_prefix}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e96606",
   "metadata": {},
   "source": [
    ">需要修改按照上述步骤的 s3url 修改 option.s3url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ea481f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile embedding_deploy_code/serving.properties\n",
    "engine=Python\n",
    "option.tensor_parallel_degree=1\n",
    "option.s3url = s3://sagemaker-us-east-1-091166060467/embedding_model/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961babb5",
   "metadata": {},
   "source": [
    ">将配置文件压缩后上传 S3 存储桶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "985d52c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "folder_path = 'embedding_deploy_code'\n",
    "output_filename = 'embedding_model.tar.gz'\n",
    "\n",
    "with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "    tar.add(folder_path, arcname=os.path.basename(folder_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fea4128",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_embedding_code_artifact = sess.upload_data(\"embedding_model.tar.gz\", bucket, s3_embedding_model_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {s3_embedding_code_artifact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccda4f7e",
   "metadata": {},
   "source": [
    "* 模型部署"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "45792342",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "\n",
    "def create_model(embedding_model_name, embedding_model_s3_url):\n",
    "    model = Model(\n",
    "        image_uri=inference_image_uri,\n",
    "        model_data=embedding_model_s3_url,\n",
    "        role=role,\n",
    "        name=embedding_model_name,\n",
    "        sagemaker_session=sess,\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8419d404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import serializers, deserializers\n",
    "\n",
    "def deploy_model(embedding_model, _embedding_endpoint_name):\n",
    "    embedding_model.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type=\"ml.g4dn.2xlarge\",\n",
    "        endpoint_name=_embedding_endpoint_name\n",
    "    )\n",
    "    predictor = sagemaker.Predictor(\n",
    "        endpoint_name=_embedding_endpoint_name,\n",
    "        sagemaker_session=sess,\n",
    "        serializer=serializers.JSONSerializer(),\n",
    "        deserializer=deserializers.JSONDeserializer()\n",
    "    )\n",
    "    return predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88669423",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "_embedding_model_name = name_from_base(f\"embedding\") # Append a timestamp to the provided string\n",
    "_embedding_model_s3_url = s3_embedding_code_artifact\n",
    "_embedding_endpoint_name = f\"{_embedding_model_name}-endpoint\"\n",
    "\n",
    "embedding_model = create_model(_embedding_model_name, _embedding_model_s3_url)\n",
    "predictor = deploy_model(embedding_model, _embedding_endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec91a334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding 模型验证\n",
    "\n",
    "prompts = \"\"\"\n",
    "北京是不是夏天雨水比较多？\n",
    "\"\"\"\n",
    "\n",
    "reponse = predictor.predict(\n",
    "    {\n",
    "        \"inputs\" : prompts\n",
    "    }\n",
    ")\n",
    "\n",
    "print(reponse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fc63ad",
   "metadata": {},
   "source": [
    "#### 7.2 通过 LangChain 使用 Embedding 模型处理文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cec298b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "from langchain.embeddings import SagemakerEndpointEmbeddings\n",
    "from langchain.embeddings.sagemaker_endpoint import EmbeddingsContentHandler\n",
    "import json\n",
    "\n",
    "class EmbeddingContentHandler(EmbeddingsContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "    \n",
    "    def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": prompt, **model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "    \n",
    "    def transform_output(self, output: bytes) -> List[List[float]]:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[\"sentence_embeddings\"]\n",
    "    \n",
    "embedding_content_handler = EmbeddingContentHandler()\n",
    "\n",
    "embeddings = SagemakerEndpointEmbeddings(\n",
    "    # credentials_profile_name=\"credentials-profile-name\",\n",
    "    endpoint_name =_embedding_endpoint_name,\n",
    "    region_name = \"us-east-1\",\n",
    "    content_handler = embedding_content_handler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05481a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证 LangChain 调用 Embedding 模型\n",
    "\n",
    "query_result = embeddings.embed_query(\"query\")\n",
    "\n",
    "doc_results = embeddings.embed_documents(['content1', 'content2'])\n",
    "\n",
    "print(query_result, '\\n\\n', doc_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb90719",
   "metadata": {},
   "source": [
    "#### 7.3 私域文档处理及私域文档 Embedding 处理后存入 Chroma 向量数据库"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf4da77",
   "metadata": {},
   "source": [
    "* 私域文档加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b92f95-9922-4c9f-a056-ad9295289d7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/terrificdm/llm-sagemaker-examples\n",
    "!mv llm-sagemaker-examples/content ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5d00b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "directory = './content'\n",
    "\n",
    "def load_docs(directory):\n",
    "  loader = DirectoryLoader(directory, show_progress=True, loader_cls=TextLoader)\n",
    "  documents = loader.load()\n",
    "  return documents\n",
    "\n",
    "documents = load_docs(directory)\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b473f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "count = 0\n",
    "for doc in documents:\n",
    "    for line in doc.page_content.split('\\n'):\n",
    "        if line.startswith('Question'):\n",
    "            count += 1\n",
    "\n",
    "print(f'Total number of questions: {count}')\n",
    "pprint.pprint(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a68283",
   "metadata": {},
   "source": [
    "* 文档切分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910e6e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"Question\"], \n",
    "    chunk_size = 0,\n",
    "    chunk_overlap = 0,\n",
    "    length_function = len,\n",
    "    # add_start_index = True,\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "pprint.pprint(docs)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a736792",
   "metadata": {},
   "source": [
    "* 部署 [Chroma 向量数据库](https://docs.trychroma.com/)，及私域文档 embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ea0fa887",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chromadb -Uq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6f0600d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "embedding_function = embeddings\n",
    "\n",
    "# Non-persistence Chroma, you can use Chroma in persistent way as described in its documents. \n",
    "db = Chroma.from_documents(docs, embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa4e61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证通过 embedding 检索私域数据\n",
    "\n",
    "query = \"沙穹秘境是什么\"\n",
    "# content = db.similarity_search(query)\n",
    "content = db.content = db.similarity_search(query, k=1)\n",
    "\n",
    "print(content[0].page_content)\n",
    "# print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f9098a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MMR Retriever\n",
    "\n",
    "# retriever = db.as_retriever(search_type=\"mmr\")\n",
    "# retriever.get_relevant_documents(query)[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fcda59f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import chromadb\n",
    "\n",
    "# client = chromadb.Client()\n",
    "# client.list_collections()\n",
    "# collection = client.get_collection(\"langchain\")\n",
    "# collection.count()\n",
    "\n",
    "# collection.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69889cf1",
   "metadata": {},
   "source": [
    "#### 7.4 构建专业问答机器人"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1e980773",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain import PromptTemplate, SagemakerEndpoint\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "import json\n",
    "\n",
    "template = \"\"\"\n",
    "你是沙穹秘境(Celestial Sands Game)的专属智能客服CelestialSandsBot，不允许谈论其他游戏，你是一个非常专业的游戏客服，请从下面三个反引号中的文档中提取相关内容，以简洁明了的方式回答用户问题，但不能以人类身份提出问题，并进行自问自答，不能随意假设答案，不能随意编造答案，如果不知道问题答案，就回答“对不起，我不知道。”\n",
    "\n",
    "```{context}```\n",
    "\n",
    "{chat_history}\n",
    "用户: {human_input}\n",
    "CelestialSandsBot:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"chat_history\", \"human_input\"], template=template\n",
    ")\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"human_input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8fe45d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlmContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "        # input_str = json.dumps({prompt: prompt, **model_kwargs})\n",
    "        input_str = json.dumps({\n",
    "                \"inputs\": prompt,\n",
    "                \"parameters\": model_kwargs,\n",
    "                \"history\":[]\n",
    "            })\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[\"outputs\"]\n",
    "\n",
    "llm_content_handler = LlmContentHandler()\n",
    "\n",
    "parameters = {\n",
    "  \"max_length\": 8192,\n",
    "  \"temperature\": 0.01,\n",
    "  \"top_p\": 0.7,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4e51e009",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = load_qa_chain(\n",
    "    llm=SagemakerEndpoint(\n",
    "        endpoint_name=_endpoint_name,\n",
    "        # credentials_profile_name=\"credentials-profile-name\",\n",
    "        region_name=\"us-east-1\",\n",
    "        model_kwargs=parameters,\n",
    "        content_handler=llm_content_handler\n",
    "    ), \n",
    "    chain_type=\"stuff\", \n",
    "    memory=memory, \n",
    "    prompt=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "abc05c84-c62d-4d2c-9d22-952e1e9cd8da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_text': '我是一个名为 CelestialSandsBot 的专属智能客服，属于沙穹秘境游戏。我的职责是回答关于游戏的问题，提供游戏相关信息和帮助。'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"你是谁？\"\n",
    "content = db.similarity_search(query)\n",
    "\n",
    "llm_chain({\"input_documents\": content, \"human_input\": query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3730346b-150f-4e69-901c-d2cca3f8afdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_text': '沙穹秘境是一款非常有趣的开放世界游戏，拥有广阔的游戏世界和多种玩法，您可以在游戏中自由探索、挑战各种任务和怪物，还可以与其他玩家互动和合作。'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"沙穹秘境好玩吗？\"\n",
    "content = db.similarity_search(query)\n",
    "\n",
    "llm_chain({\"input_documents\": content, \"human_input\": query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "23920b5e-24c0-4a2f-ac22-13e42feb91d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_text': '沙穹秘境中有各种各样的商店，包括武器店、防具店、杂货店、宠物店等等。您可以在这些商店中购买各种装备、道具、宠物、坐骑、时装等物品。'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"沙穹秘境中有哪些商店？\"\n",
    "content = db.similarity_search(query)\n",
    "\n",
    "llm_chain({\"input_documents\": content, \"human_input\": query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4dad5823",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_text': '在沙穹秘境中，您可以使用各种武器和技能来对敌人进行攻击。攻击的方式包括普通攻击、技能攻击、投掷物攻击等。\\n\\n普通攻击：您可以通过鼠标左键点击敌人来进行普通攻击。技能攻击：您可以通过鼠标右键点击敌人来释放技能攻击。投掷物攻击：您可以通过按住鼠标左键并拖动鼠标来投掷物品攻击敌人。\\n\\n攻击时需要注意以下几点：\\n\\n1. 攻击力会根据您的角色等级和装备进行调整。\\n2. 某些敌人可能具有防御性或抵抗性，需要根据实际情况来选择攻击方式。\\n3. 攻击时要注意血量和魔法值，避免因血量不足或魔法值不足而导致无法继续攻击。'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"如何攻击别人？\"\n",
    "content = db.similarity_search(query)\n",
    "\n",
    "llm_chain({\"input_documents\": content, \"human_input\": query}, return_only_outputs=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
